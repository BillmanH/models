{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c72bc34eed9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import matplotlib \n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyLDAvis\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 10000\n",
    "n_topics = 10\n",
    "n_top_words = 100\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "mystopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will set up the document using the .txt files that you have in sharepoint, below is the version for excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3862bf4e1280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\".txt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mHealthcare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"Health\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "#MyPath = r\"C:\\Users\\williamh\\OneDrive - METIA LTD\\LinkedIn\\{}\" #for running this code on my work PC\n",
    "MyPath = \"\"\n",
    "\n",
    "\n",
    "files = [file for file in os.listdir(MyPath.format(\"\")) if \".txt\" in file]\n",
    "\n",
    "Healthcare = [file for file in files if \"Health\" in file]\n",
    "Finance = [file for file in files if \"Finance\" in file]\n",
    "\n",
    "hdf = {}\n",
    "fdf = {}\n",
    "alldf = {}\n",
    "\n",
    "for file in Healthcare:\n",
    "    print(file)\n",
    "    with open (MyPath.format(file), \"r\") as myfile:\n",
    "        data=\"\".join([line.rstrip('\\n') for line in myfile])\n",
    "    i = len(hdf)\n",
    "    hdf[file] = data\n",
    "    \n",
    "for file in Finance:\n",
    "    print(file)\n",
    "    with open (MyPath.format(file), \"r\") as myfile:\n",
    "        data=\"\".join([line.rstrip('\\n') for line in myfile])\n",
    "    i = len(fdf)\n",
    "    fdf[file] = data\n",
    "    \n",
    "for file in files:\n",
    "    print(file)\n",
    "    with open (MyPath.format(file), \"r\") as myfile:\n",
    "        data=\"\".join([line.rstrip('\\n') for line in myfile])\n",
    "    i = len(alldf)\n",
    "    alldf[file] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_excel('Copy of LinkedIn_InterviewCategorization_111816.xlsx')\n",
    "df = pd.read_excel('Copy of LinkedIn_InterviewCategorization_1118113016.xlsx')\n",
    "df.index = df['Interview']\n",
    "df = df.drop(['Interview'],axis=1)\n",
    "df = df.replace(np.nan,' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Background &amp; Work Environment</th>\n",
       "      <th>Challenges</th>\n",
       "      <th>Roles of Content</th>\n",
       "      <th>Types of Content</th>\n",
       "      <th>Content Consumption Habits</th>\n",
       "      <th>Live Community</th>\n",
       "      <th>Social</th>\n",
       "      <th>LinkedIn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interview</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nov. 9, 2016 – Interview 5 - Finance</th>\n",
       "      <td>Sure. I knew I was always going to be an accou...</td>\n",
       "      <td>I think getting people to understand what I'm ...</td>\n",
       "      <td>Oh, it plays a huge role. It changes every day...</td>\n",
       "      <td>Sure. I mean if we're talking about looking up...</td>\n",
       "      <td>on my phone, I can ask Siri just about anythin...</td>\n",
       "      <td>Oh, I highly believe it's not what you know, i...</td>\n",
       "      <td>But if I'm looking for something specific, I u...</td>\n",
       "      <td>Yeah, I use it to... Well, one, I do monitor e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov 11, 2016 - Interview 6 - Finance</th>\n",
       "      <td>With 1.5 billion. I’m an equity portfolio mana...</td>\n",
       "      <td>One of the big drawbacks of Google right now i...</td>\n",
       "      <td>The reasons would be obviously independent of ...</td>\n",
       "      <td>I guess for information, maybe just a little b...</td>\n",
       "      <td>It’s not going to sound like a lot; but again,...</td>\n",
       "      <td></td>\n",
       "      <td>but one of the really impressive things with T...</td>\n",
       "      <td>Yes. The best that I can. For the most part, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov 9, 2016 - Interview 5 - Healthcare</th>\n",
       "      <td>I am Associate Professor of Surgery in the Uni...</td>\n",
       "      <td>Yes, time restrictions. The schedule is gettin...</td>\n",
       "      <td>You want specifics, what I use, or how importa...</td>\n",
       "      <td>Well, let’s see what type of resources. If we ...</td>\n",
       "      <td>If I have specific devices, you said, also? Ye...</td>\n",
       "      <td></td>\n",
       "      <td>I will not discuss about Facebook because I do...</td>\n",
       "      <td>So I am like a member of LinkedIn for the past...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov 10, 2016 - Interview 8 - Healthcare</th>\n",
       "      <td>Sure, so I deliver anesthesia. I’m board-certi...</td>\n",
       "      <td>I think the big one is the shift in culture an...</td>\n",
       "      <td>Sure. Well so, in healthcare there’s seven dif...</td>\n",
       "      <td>Well, the history behind it is about 20 years ...</td>\n",
       "      <td>Oh yeah, all the time. I’m always hooked in. \\...</td>\n",
       "      <td>Well, we’re looking at two different versions ...</td>\n",
       "      <td>… I teach my grad students when it comes to or...</td>\n",
       "      <td>I’m just the basic professional profile. I rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov 10, 2016 - Interview 9 - Healthcare</th>\n",
       "      <td>…And went on to work in an outpatient setting ...</td>\n",
       "      <td>Some of the challenges that I face in my role…...</td>\n",
       "      <td>Obtaining content or information, if I have a ...</td>\n",
       "      <td>We use the UpToDate resource manuals. I don’t ...</td>\n",
       "      <td>Sure. Online I just feel it’s at the tip of my...</td>\n",
       "      <td>Say I’m having a really tough day. I know nurs...</td>\n",
       "      <td>I know one of the topics that was in the e-mai...</td>\n",
       "      <td>No. To be honest with you, no. I remember for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             Background & Work Environment  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     Sure. I knew I was always going to be an accou...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     With 1.5 billion. I’m an equity portfolio mana...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   I am Associate Professor of Surgery in the Uni...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  Sure, so I deliver anesthesia. I’m board-certi...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  …And went on to work in an outpatient setting ...   \n",
       "\n",
       "                                                                                Challenges  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     I think getting people to understand what I'm ...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     One of the big drawbacks of Google right now i...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   Yes, time restrictions. The schedule is gettin...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  I think the big one is the shift in culture an...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  Some of the challenges that I face in my role…...   \n",
       "\n",
       "                                                                          Roles of Content  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     Oh, it plays a huge role. It changes every day...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     The reasons would be obviously independent of ...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   You want specifics, what I use, or how importa...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  Sure. Well so, in healthcare there’s seven dif...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  Obtaining content or information, if I have a ...   \n",
       "\n",
       "                                                                          Types of Content  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     Sure. I mean if we're talking about looking up...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     I guess for information, maybe just a little b...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   Well, let’s see what type of resources. If we ...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  Well, the history behind it is about 20 years ...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  We use the UpToDate resource manuals. I don’t ...   \n",
       "\n",
       "                                                                Content Consumption Habits  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     on my phone, I can ask Siri just about anythin...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     It’s not going to sound like a lot; but again,...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   If I have specific devices, you said, also? Ye...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  Oh yeah, all the time. I’m always hooked in. \\...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  Sure. Online I just feel it’s at the tip of my...   \n",
       "\n",
       "                                                                            Live Community  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     Oh, I highly believe it's not what you know, i...   \n",
       "Nov 11, 2016 - Interview 6 - Finance                                                         \n",
       "Nov 9, 2016 - Interview 5 - Healthcare                                                       \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  Well, we’re looking at two different versions ...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  Say I’m having a really tough day. I know nurs...   \n",
       "\n",
       "                                                                                    Social  \\\n",
       "Interview                                                                                    \n",
       "Nov. 9, 2016 – Interview 5 - Finance     But if I'm looking for something specific, I u...   \n",
       "Nov 11, 2016 - Interview 6 - Finance     but one of the really impressive things with T...   \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   I will not discuss about Facebook because I do...   \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  … I teach my grad students when it comes to or...   \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  I know one of the topics that was in the e-mai...   \n",
       "\n",
       "                                                                                  LinkedIn  \n",
       "Interview                                                                                   \n",
       "Nov. 9, 2016 – Interview 5 - Finance     Yeah, I use it to... Well, one, I do monitor e...  \n",
       "Nov 11, 2016 - Interview 6 - Finance     Yes. The best that I can. For the most part, I...  \n",
       "Nov 9, 2016 - Interview 5 - Healthcare   So I am like a member of LinkedIn for the past...  \n",
       "Nov 10, 2016 - Interview 8 - Healthcare  I’m just the basic professional profile. I rea...  \n",
       "Nov 10, 2016 - Interview 9 - Healthcare  No. To be honest with you, no. I remember for ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortcorp = {}\n",
    "\n",
    "for row in df.index:\n",
    "    for col in df.columns.tolist():\n",
    "        idx = row +'::'+ col\n",
    "        sortcorp[idx] = df.loc[row,col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing this for one corpus at a time:\n",
    "change this line to switch between corpus\n",
    "\n",
    "\n",
    "I added some new stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = sortcorp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_samples = corpus.values()\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                lowercase= True,\n",
    "                                token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=mystopwords,\n",
    "                                max_df=0.5,\n",
    "                                min_df=10)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=10000...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bunch of functions that I wrote to build the tables for the model:\n",
    "def get_single_topic(lda, tf_feature_names, n_top_words, topic):\n",
    "    words = [tf_feature_names[i] for i in lda.components_[topic].argsort()[:-n_top_words - 1:-1]]\n",
    "    scores = lda.components_[topic][lda.components_[topic].argsort()[:-n_top_words - 1:-1]]\n",
    "    df = pd.DataFrame(index=words,columns=['topic_{}'.format(topic)],data=scores)\n",
    "    return df\n",
    "\n",
    "def get_all_topics(lda, tf_feature_names, n_top_words,n_topics):\n",
    "    df = pd.DataFrame()\n",
    "    for topic in range(n_topics):\n",
    "        tmpdf = get_single_topic(lda, tf_feature_names, n_top_words, topic)\n",
    "        for item in tmpdf.index:\n",
    "            df.loc[item,'topic_{}'.format(topic)] = tmpdf.loc[item,'topic_{}'.format(topic)]\n",
    "    return df\n",
    "\n",
    "def get_topic_names(lda, tf_feature_names, n_top_words, n_topics):\n",
    "    themes = pd.Series()\n",
    "    for topic in range(n_topics):\n",
    "        theme = \" \".join([tf_feature_names[i] for i in lda.components_[topic].argsort()[:-n_top_words - 1:-1]])\n",
    "        themes.loc['topic_{}'.format(topic)] = theme\n",
    "    return themes\n",
    "\n",
    "words = get_all_topics(lda, tf_feature_names, n_top_words,n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_document(doc_dic,df,lda, tf_feature_names, n_top_words, n_topics,\n",
    "                    returnDF=True,confidence=.01):\n",
    "    '''\n",
    "    gives scores to the origional document, assigning a category to each one. \n",
    "\n",
    "    returnDF : By default returns a DataFrame, set to false to return a dict.\n",
    "    confidence : this is the threashold that the model must meet to match the document to a topic.\n",
    "    set to .01 to include practically everything, set to .99 to include almost nothing.\n",
    "\n",
    "    document_scores = score_document(doc_dic,df,lda, tf_feature_names, n_top_words, n_topics)\n",
    "    '''\n",
    "    results_dict = {}\n",
    "    for key, item in enumerate(doc_dic.keys()):\n",
    "        document = doc_dic[item]\n",
    "        words = [tf_feature_names[i] for i in tf.getrow(list(doc_dic).index(item)).indices]\n",
    "        scores = df[[word in words for word in df.index]]\n",
    "        TM_Score = pd.DataFrame()\n",
    "        TM_Score['docScore'] = scores.sum() \n",
    "        TM_Score['globalScore'] = df.sum()\n",
    "        TM_Score['relevance'] = TM_Score['docScore']/TM_Score['globalScore']\n",
    "        TM_Score['theme'] = get_topic_names(lda, tf_feature_names, n_top_words, n_topics)\n",
    "        results = TM_Score['relevance'].fillna(0).to_dict()\n",
    "        results['document'] = document\n",
    "        results['key'] = item\n",
    "        results['top_score'] = TM_Score['relevance'].max()\n",
    "\n",
    "        if TM_Score['relevance'].max() >= confidence:\n",
    "            results['top_theme'] = TM_Score['theme'][TM_Score['relevance'].tolist().index(TM_Score['relevance'].max())]\n",
    "            results['top_topic'] = TM_Score.index[TM_Score['relevance'].tolist().index(TM_Score['relevance'].max())]\n",
    "        else: \n",
    "            results['top_theme'] = 'unassigned'\n",
    "            results['top_topic'] = 'unassigned'\n",
    "        results_dict[key] = results\n",
    "    if returnDF:\n",
    "        return pd.DataFrame(results_dict).T\n",
    "    else:\n",
    "        return results_dict\n",
    "\n",
    "    \n",
    "Scored_corpus = score_document(corpus,words,lda, tf_feature_names, n_top_words, n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>key</th>\n",
       "      <th>top_score</th>\n",
       "      <th>top_theme</th>\n",
       "      <th>top_topic</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>interveiw</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They are rather a pretty important part, like...</td>\n",
       "      <td>November 7th, 2016 - Interview 1 - Finance Man...</td>\n",
       "      <td>0.792395</td>\n",
       "      <td>like would think kind information know really ...</td>\n",
       "      <td>topic_9</td>\n",
       "      <td>0.523273</td>\n",
       "      <td>0.653863</td>\n",
       "      <td>0.65777</td>\n",
       "      <td>0.157526</td>\n",
       "      <td>0.630594</td>\n",
       "      <td>0.682426</td>\n",
       "      <td>0.600344</td>\n",
       "      <td>0.749704</td>\n",
       "      <td>0.775597</td>\n",
       "      <td>0.792395</td>\n",
       "      <td>November 7th, 2016 - Interview 1 - Finance Man...</td>\n",
       "      <td>Roles of Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well my current role is that I've been an owne...</td>\n",
       "      <td>November 8th, 2016 - Interview 2 - Financial P...</td>\n",
       "      <td>0.278393</td>\n",
       "      <td>like would think get kind know different infor...</td>\n",
       "      <td>topic_8</td>\n",
       "      <td>0.259493</td>\n",
       "      <td>0.219581</td>\n",
       "      <td>0.224594</td>\n",
       "      <td>0.14637</td>\n",
       "      <td>0.252046</td>\n",
       "      <td>0.218524</td>\n",
       "      <td>0.176497</td>\n",
       "      <td>0.232671</td>\n",
       "      <td>0.278393</td>\n",
       "      <td>0.220622</td>\n",
       "      <td>November 8th, 2016 - Interview 2 - Financial P...</td>\n",
       "      <td>Background &amp; Work Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh yeah, all the time. I’m always hooked in. \\...</td>\n",
       "      <td>Nov 10, 2016 - Interview 8 - Healthcare::Conte...</td>\n",
       "      <td>0.827682</td>\n",
       "      <td>like would think kind information know really ...</td>\n",
       "      <td>topic_9</td>\n",
       "      <td>0.763549</td>\n",
       "      <td>0.610349</td>\n",
       "      <td>0.644218</td>\n",
       "      <td>0.109392</td>\n",
       "      <td>0.477082</td>\n",
       "      <td>0.746416</td>\n",
       "      <td>0.622737</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.814217</td>\n",
       "      <td>0.827682</td>\n",
       "      <td>Nov 10, 2016 - Interview 8 - Healthcare</td>\n",
       "      <td>Content Consumption Habits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>… I teach my grad students when it comes to or...</td>\n",
       "      <td>Nov 10, 2016 - Interview 8 - Healthcare::Social</td>\n",
       "      <td>0.690475</td>\n",
       "      <td>know people think something would use informat...</td>\n",
       "      <td>topic_5</td>\n",
       "      <td>0.636644</td>\n",
       "      <td>0.524978</td>\n",
       "      <td>0.526024</td>\n",
       "      <td>0.302186</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>0.690475</td>\n",
       "      <td>0.574269</td>\n",
       "      <td>0.575996</td>\n",
       "      <td>0.595668</td>\n",
       "      <td>0.600739</td>\n",
       "      <td>Nov 10, 2016 - Interview 8 - Healthcare</td>\n",
       "      <td>Social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A lot of the time also, if I feel like I use r...</td>\n",
       "      <td>Nov. 8, 2016 – Interview 7 - Healthcare::Live ...</td>\n",
       "      <td>0.36862</td>\n",
       "      <td>like would think get kind know different infor...</td>\n",
       "      <td>topic_8</td>\n",
       "      <td>0.207338</td>\n",
       "      <td>0.261229</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.0481338</td>\n",
       "      <td>0.255973</td>\n",
       "      <td>0.262299</td>\n",
       "      <td>0.247508</td>\n",
       "      <td>0.326976</td>\n",
       "      <td>0.36862</td>\n",
       "      <td>0.340126</td>\n",
       "      <td>Nov. 8, 2016 – Interview 7 - Healthcare</td>\n",
       "      <td>Live Community</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0   They are rather a pretty important part, like...   \n",
       "1  Well my current role is that I've been an owne...   \n",
       "2  Oh yeah, all the time. I’m always hooked in. \\...   \n",
       "3  … I teach my grad students when it comes to or...   \n",
       "4  A lot of the time also, if I feel like I use r...   \n",
       "\n",
       "                                                 key top_score  \\\n",
       "0  November 7th, 2016 - Interview 1 - Finance Man...  0.792395   \n",
       "1  November 8th, 2016 - Interview 2 - Financial P...  0.278393   \n",
       "2  Nov 10, 2016 - Interview 8 - Healthcare::Conte...  0.827682   \n",
       "3    Nov 10, 2016 - Interview 8 - Healthcare::Social  0.690475   \n",
       "4  Nov. 8, 2016 – Interview 7 - Healthcare::Live ...   0.36862   \n",
       "\n",
       "                                           top_theme top_topic   topic_0  \\\n",
       "0  like would think kind information know really ...   topic_9  0.523273   \n",
       "1  like would think get kind know different infor...   topic_8  0.259493   \n",
       "2  like would think kind information know really ...   topic_9  0.763549   \n",
       "3  know people think something would use informat...   topic_5  0.636644   \n",
       "4  like would think get kind know different infor...   topic_8  0.207338   \n",
       "\n",
       "    topic_1   topic_2    topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0  0.653863   0.65777   0.157526  0.630594  0.682426  0.600344  0.749704   \n",
       "1  0.219581  0.224594    0.14637  0.252046  0.218524  0.176497  0.232671   \n",
       "2  0.610349  0.644218   0.109392  0.477082  0.746416  0.622737  0.807018   \n",
       "3  0.524978  0.526024   0.302186  0.508312  0.690475  0.574269  0.575996   \n",
       "4  0.261229  0.281715  0.0481338  0.255973  0.262299  0.247508  0.326976   \n",
       "\n",
       "    topic_8   topic_9                                          interveiw  \\\n",
       "0  0.775597  0.792395  November 7th, 2016 - Interview 1 - Finance Man...   \n",
       "1  0.278393  0.220622  November 8th, 2016 - Interview 2 - Financial P...   \n",
       "2  0.814217  0.827682            Nov 10, 2016 - Interview 8 - Healthcare   \n",
       "3  0.595668  0.600739            Nov 10, 2016 - Interview 8 - Healthcare   \n",
       "4   0.36862  0.340126            Nov. 8, 2016 – Interview 7 - Healthcare   \n",
       "\n",
       "                    conversation  \n",
       "0               Roles of Content  \n",
       "1  Background & Work Environment  \n",
       "2     Content Consumption Habits  \n",
       "3                         Social  \n",
       "4                 Live Community  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scored_corpus['interveiw'] = Scored_corpus['key'].apply(lambda x: x.split(\"::\")[0])\n",
    "Scored_corpus['conversation'] = Scored_corpus['key'].apply(lambda x: x.split(\"::\")[1])\n",
    "Scored_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_0    information would lot kind one get going diffe...\n",
       "topic_1    get well one website information think know lo...\n",
       "topic_2    like linkedin would know facebook think kind r...\n",
       "topic_3    think material gaining highly 18 accomplishmen...\n",
       "topic_4    kind like know would things really search work...\n",
       "topic_5    know people think something would use informat...\n",
       "topic_6    kind really linkedin think like know lot use o...\n",
       "topic_7    like people go think things would something ge...\n",
       "topic_8    like would think get kind know different infor...\n",
       "topic_9    like would think kind information know really ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_names(lda, tf_feature_names, n_top_words, n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Output to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words.to_csv(r'token_list.csv')\n",
    "\n",
    "Scored_corpus.to_csv(r'Scored_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
